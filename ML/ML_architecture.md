# ML - Architecture
|Paper|Conference|Remarks
|--|--|--|
|[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555)|Arxiv 2014|Evaluate LSTM and GRU on the tasks of polyphonic music modeling and speech signal modeling and results show that they indeed perform better than traditional recurrent units|
|[Neural Turing Machines](https://arxiv.org/pdf/1410.5401)|Arxiv 2014|Coupling neural networks to external memory resources, which they can interact with by attentional processes.|
|[Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/pdf/1506.03099)|NIPS 2015|To alleviate the discrepancy between training and inference for sequence prediction models, authors propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead|
|[Professor Forcing: A New Algorithm for Training Recurrent Networks](https://arxiv.org/pdf/1610.09038)|NIPS 2016|1. Uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. 2. Produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.|
|[Hybrid Computing Using a Neural Network with Dynamic External Memory](https://www.nature.com/articles/nature20101)|Nature 2016| Traditional neural networks are limited in their ability to represent variables and data structures and to store data over long timescales owing the lack of an external memory. This paper proposes a machine learning model called a differential neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the RAM in a conventional computer.|
|[Style Transfer from Non-Parallel Text by Cross-Alignment](https://arxiv.org/pdf/1705.09655)|NIPS 2017| 1. Introduce an seq2seq architecture based entirely on convolutional neural networks. 2. Compared to recurrent models, the proposed architecture has the advantage of fully parallelization during training and easier optimization since the number of non-linearities is fixed and independent of the input length. 3. Use gated linear units to ease gradient propagation and equip each decoder layer with a separate attention module.|
|[Neural Text Generation: A Practical Guide]
|[Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122)|Arxiv 2017| 1. Introduce an seq2seq architecture based entirely on convolutional neural networks. 2. Compared to recurrent models, the proposed architecture has the advantage of fully parallelization during training and easier optimization since the number of non-linearities is fixed and independent of the input length. 3. Use gated linear units to ease gradient propagation and equip each decoder layer with a separate attention module.|
|[Neural Text Generation: A Practical Guide](https://arxiv.org/pdf/1711.09534)|Arxiv 2017| 1. Current neural generative models suffer from generating truncated or repetitive outputs, outputting bland and generic responses, or in some cases producing ungrammatical gibberish. 2. Give a practical guide for resolving such undesired behavior in text generation models, with the aim of helping enable real-world applications.|
|[Recent Advances in Recurrent Neural Networks](https://arxiv.org/pdf/1801.01078)|Arxiv 2017| 1. Present a survey on RNNs and several new advances. 2. The fundamentals and recent advances are explained and the research challenges are introduced.|
|[Dynamic Evaluation of Neural Sequence Models](http://proceedings.mlr.press/v80/krause18a/krause18a.pdf)|ICML 2018| 1. Explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher probabilities to re-occurring sequential patterns. 2. Achieve the state-of-the-art results in language modelling|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIyODE5NTI0MywtMTI1ODE3OTA2OCwtMT
E1NTQzNjQ4OV19
-->