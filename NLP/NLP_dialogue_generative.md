# NLP - Dialogue System (Open-domain, Generative)
|Paper|Conference|Remarks
|--|--|--|
|[A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](https://arxiv.org/pdf/1506.06714)|NAACL 2015|Integrate context information into dialogue generation|
|[A Neural Conversational Model](https://arxiv.org/pdf/1506.05869)|Arxiv 2015|1. Present a simple approach for conversation modelling which uses the recently proposed sequence to sequence framework. 2. It can be trained end-to-end and thus requires much fewer hand-crafted rules|
|[A Persona-Based Neural Conversation Model](http://www.aclweb.org/anthology/P16-1094)|ACL 2016|1. Present persona-based models for handling the issue of speaker consistency in neural response generation. 2. Yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.|
|[Deep Reinforcement Learning for Dialogue Generation](https://aclweb.org/anthology/D16-1127)|EMNLP 2016|1. Current conversational agents tend to be short-sighted and ignoring long-term interactivity. 2. Combination of traditional NLP model with reinforcement learning to model future rewards in chatbot dialogue to learn a neural conversational model based on long-term success of dialogues. 3. Rewards: informative, coherence and ease-of-answering. 4. Model evaluation metrices: diversity, conversation length and human judges|
|[How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation](https://aclweb.org/anthology/D16-1230)|EMNLP 2016|1. Investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. 2. Show that current automatic metrics correlate very weakly with human judgement in the non-technical Twitter domain, and not at all in the technical Ubuntu domain|
|[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)|AAAI 2016|1. Extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. 2. Investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings|
|[Topic Aware Neural Response Generation](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14563/14260)|AAAI 2016|1. Consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. 2. Utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability.|
|[A Context-aware Natural Language Generator for Dialogue Systems](https://arxiv.org/pdf/1608.07076)|SIGDIAL 2016|Present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses.|
|[A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/pdf/1611.08562)|Arxiv 2016|1. Propose a simple, fast decoding algorithm that fosters diversity in neural generation. 2. Diverse decoding helps across tasks of dialogue response generation, abstractive summarization and machine translation, especially those for which reranking is needed|
|[A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues](https://arxiv.org/pdf/1605.06069)|Arxiv 2016|1. Propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps to model complex dependencies between subsequences.|
|[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424)|Arxiv 2016|1. Propose DBS to decode a list of diverse outputs by optimizing for a diversity-augmented objective. 2. Observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space|
|[Generative Deep Neural Networks for Dialogue: A Short Review](https://arxiv.org/pdf/1611.06216)|Arxiv 2016|Review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.|
|[Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders](https://arxiv.org/pdf/1703.10960)|ACL 2017| 1. Present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. The proposed model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. 2. Further develop a novel variant that is integrated with linguistic prior knowledge for better performance. 3. Propose a bag-of-word loss to alleviate the problem of vanishing KL loss.|
|[Adversarial Learning for Neural Dialogue Generation](https://www.aclweb.org/anthology/D17-1230)|EMNLP 2017| 1. Propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. 2. Cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. 3. Some appealing research directions.|
|[Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models](https://arxiv.org/pdf/1701.03185)|EMNLP 2017| 1. Add self-attention to the decoder to maintain coherence in longer responses, and propose a practical approach, called the glimpse-model, for scaling to large datasets. 2. Introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process.|
|[ParlAI: A Dialog Research Software Platform](http://aclweb.org/anthology/D17-2014)|EMNLP 2017| A unified open-source framework in Python for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others' models, and improving upon existing architectures.|
|[A Survey on Dialogue Systems: Recent Advances and New Frontiers](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Poetry Generation](https://pdfs.semanticscholar.org/d89d/053b1c2481088b1af2bd36e0a6d959ff1373.pdf)|SPECOM 2017| 1. Describe a new system for the automatic creation of poetry in Basque that not only generates novel poems, but also creates them conveying a certain attitude or state of mind. 2. The proposed system receives as an input the topic of the poem and the affective state (positive, neutral or negative) and tries to give as output a novel poem that: (1) satisfies formal constraints of rhyme and metric, (2) shows coherent content related to the given topic, and (3) expresses them through the predetermined mood.|
|[Emotional Human-Machine Conversation Generation Based on Long Short-Term Memory](https://link.springer.com/article/10.1007/s12559-017-9539-4)|Cognitive Computation 2017| 1. Propose a new model based on long short-term memory, which is used to achieve an encoder-decoder framework, and we address the emotional factor of conversation generation by changing the model’s input using a series of input transformations: a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.|
|[A Deep Reinforcement Learning Chatbot](https://arxiv.org/pdf/1709.02349)|Arxiv 2017| 1. Consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. 2. The system has been trained to select an appropriate response from the models in its ensemble.|
|[Building Chatbot with Emotions](http://web.stanford.edu/class/cs224s/reports/Honghao_Wei.pdf)|N.A. 2017| 1. Aims at generating dialogues not only appropriate at content level, but also containing specific emotions. 2. Apply sentimental analysis on the dataset and pick up dialogue with strong emotion. 3. Apply deep reinforcement learning and introduce sentiment rewards during learning phase|
|[Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base](https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf)|NIPS 2018|1. Present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. 2. Introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances.|
|[Knowledge Diffusion for Neural Dialogue Generation](http://www.aclweb.org/anthology/P18-1138)|ACL 2018|1. Propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. 2. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities.|
|[Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures](http://aclweb.org/anthology/P18-1133)|ACL 2018|1. Propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. 2. Design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. 3. Propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude.|
|[Sentiment Adaptive End-to-End Dialog Systems](https://arxiv.org/pdf/1804.10731)|ACL 2018|1. Propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end dialog learning framework to make systems more user-adaptive and effective. 2. Incorporated user sentiment information in both supervised and reinforcement learning settings and gained improvements on a bus information search task.|
|[MojiTalk: Generating Emotional Responses at Scale](https://arxiv.org/pdf/1711.04090)|ACL 2018|1. Collect a large corpus of Twitter conversations that include emojis in the response, and assume the emojis convey the underlying emotions of the sentence. 2. Introduce a reinforced conditional variational encoder approach to train a deep generative model on these conversations, which allows us to use emojis to control the emotion of the generated text.|
|[Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems](https://arxiv.org/pdf/1804.08217)|ACL 2018|Propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to incorporate knowledge bases. |
|[Global-Locally Self-Attentive Dialogue State Tracker](https://arxiv.org/pdf/1805.09655)|ACL 2018|1. Propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. 2. The model uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. 3. Show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks.|
|[Variational Autoregressive Decoder for Neural Response Generation](http://aclweb.org/anthology/D18-1354)|EMNLP 2018|1. To solve the generation problem in VAE, this work proposes a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. 2. The approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation.|
|[Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](http://www.aclweb.org/anthology/N18-1193)|NAACL 2018|Propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history, in dyadic dialogue videos|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018|1. To address the KL-vanishing problem and inconsistent training objective in VAE, they propose to separate the training step into two phases: The first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Augmenting End-to-End Dialogue Systems with Commonsense Knowledge](https://arxiv.org/pdf/1709.05453)|AAAI 2018| 1. Investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. 2. Propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response|
|[A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/pdf/1702.01932)|AAAI 2018| 1. Presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. 2. Generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting.|
|[Neural Response Generation with Dynamic Vocabularies](https://arxiv.org/pdf/1711.11191)|AAAI 2018| 1. Propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. 2. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. 3. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary.|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018| In VAE, encoder and decoder training are inconsistent, this work separate two phases: the first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[A Hierarchical Latent Structure for Variational Conversation Modeling](https://arxiv.org/pdf/1804.03424)|NAACL 2018| 1. VAE for dialogue generation suffers from the generation problems: (1) the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables; (2) the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. 2. To solve the generation problem, this paper proposes a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization.|
|[Dialog Generation Using Multi-Turn Reasoning Neural Networks](https://arxiv.org/pdf/1804.03424)|NAACL 2018| Propose a generalizable dialog generation approach that adapts multiturn reasoning, one recent advancement in the field of document comprehension, to generate responses (“answers”) by taking current conversation session context as a “document” and current query as a “question”.|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|
|[Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models](https://arxiv.org/pdf/1708.03152)|LREC 2018| 1. Propose speaker classification as a surrogate task for general speaker modeling, and collect massive data to facilitate research in this direction.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|
|[Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References](https://www.aclweb.org/anthology/P19-1372.pdf)|ACL 2019| 1. Propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. 2. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses.|
|[Dialogue Natural Language Inference](https://arxiv.org/pdf/1811.00671)|ACL 2019| 1. Frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. 2. Propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model.|
|[Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study](https://arxiv.org/pdf/1906.01603)|ACL 2019| 1. Take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. 2. Experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc.|
|[Domain Adaptive Dialog Generation via Meta Learning](https://arxiv.org/pdf/1906.03520)|ACL 2019| 1. Propose a domain adaptive dialog generation method based on meta-learning (DAML), an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. 2. Train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain.|
|[DyKgChat - Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs](https://arxiv.org/pdf/1910.00610)|EMNLP 2019| 1. Proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. 2. Proposes a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs.|
|[Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models]([http://proceedings.mlr.press/v97/cohen19a.html](http://proceedings.mlr.press/v97/cohen19a.html))|ICML 2019| 1. Perform an empirical study of the behavior of beam search across three sequence synthesis tasks. 2. Find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. 3. Show that constraining beam search to avoid large discrepancies eliminates the performance degradation.|
|[Gunrock - A Social Bot for Complex and Engaging Long Conversations](https://arxiv.org/pdf/1910.03042)|EMNLP 2019| 1. The winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazon-selected expert conversationalists. 2. Introduce some innovative system designs and related validation analysis.|
|[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://arxiv.org/pdf/1906.07004)|ACL 2019| 1. Propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. 2. Collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network.|
|[Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs](https://arxiv.org/pdf/1903.10245)|EMNLP 2019| 1. Propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. 2. Formulate the knowledge selection on the graph as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous work.|
|[Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation](https://www.aclweb.org/anthology/N19-1349/)|NAACL 2019| 1. Examine whether specificity is solely a frequency-related notion and find that more linguistically-driven specificity measures are better suited to improving response informativeness. 2. Find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics.|
|[OpenDialKG - Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs](https://research.fb.com/wp-content/uploads/2019/07/OpenDialKG-Explainable-Conversational-Reasoning-with-Attention-based-Walks-over-Knowledge-Graphs.pdf?)|ACL 2019| 1. Collect a new **Open**-ended **Dial**og ↔ **KG** parallel corpus called _OpenDialKG_, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. 2. Propose the _DialKG Walker_ model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder.|
|[Personalizing Dialogue Agents via Meta-Learning](https://arxiv.org/pdf/1905.10033)|ACL 2019| 1. Propose to extend Model-Agnostic Meta-Learning (MAML)(Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. 2. The model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions.|
|[Persuasion for Good - Towards a Personalized Persuasive Dialogue System for Social Good](https://arxiv.org/pdf/1906.06725)|ACL 2019| 1. Design an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. 2. Collect a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. 3. Build a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. 4. Analyze the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation.|
|[Structuring Latent Spaces for Stylized Response Generation](https://arxiv.org/pdf/1909.05361)|EMNLP 2019| 1. Propose StyleFusion, which bridges conversation modeling and non-parallel style transfer by sharing a structured latent space. 2. This structure allows the system to generate stylized relevant responses by sampling in the neighborhood of the conversation model prediction, and continuously control the style level.|
|[The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751)|ICLR 2020| 1. Reveal surprising distributional differences between human text and machine text. 2. Find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. 3. Propose Nucleus Sampling, a simple but effective method to draw the best out of neural generation.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|


[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbNzAyNzg4NDUzLC00MDgzMTg0NjBdfQ==
-->