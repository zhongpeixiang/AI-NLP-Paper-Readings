# NLP - Dialogue System
|Paper|Conference|Remarks
|--|--|--|
|[POMDP-based Statistical Spoken Dialogue Systems: a Review](http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf)|Proceedings of IEEE 2013|1. By including an explicit Bayesian model of uncertainty and by optimizing the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide a data-driven framework for dialog modelling. 2. Provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.|
|[A Neural Conversational Model](https://arxiv.org/pdf/1506.05869)|Arxiv 2015|1. Present a simple approach for conversation modelling which uses the recently proposed sequence to sequence framework. 2. It can be trained end-to-end and thus requires much fewer hand-crafted rules|
|[A Persona-Based Neural Conversation Model](http://www.aclweb.org/anthology/P16-1094)|ACL 2016|1. Present persona-based models for handling the issue of speaker consistency in neural response generation. 2. Yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.|
|[Deep Reinforcement Learning for Dialogue Generation](https://aclweb.org/anthology/D16-1127)|EMNLP 2016|1. Current conversational agents tend to be short-sighted and ignoring long-term interactivity. 2. Combination of traditional NLP model with reinforcement learning to model future rewards in chatbot dialogue to learn a neural conversational model based on long-term success of dialogues. 3. Rewards: informative, coherence and ease-of-answering. 4. Model evaluation metrices: diversity, conversation length and human judges|
|[How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation](https://aclweb.org/anthology/D16-1230)|EMNLP 2016|1. Investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. 2. Show that current automatic metrics correlate very weakly with human judgement in the non-technical Twitter domain, and not at all in the technical Ubuntu domain|
|[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)|AAAI 2016|1. Extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. 2. Investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings|
|[Topic Aware Neural Response Generation](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14563/14260)|AAAI 2016|1. Consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. 2. Utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability.|
|[A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/pdf/1611.08562)|Arxiv 2016|1. Propose a simple, fast decoding algorithm that fosters diversity in neural generation. 2. Diverse decoding helps across tasks of dialogue response generation, abstractive summarization and machine translation, especially those for which reranking is needed|
|[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424)|Arxiv 2016|1. Propose DBS to decode a list of diverse outputs by optimizing for a diversity-augmented objective. 2. Observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space|
|[Generative Deep Neural Networks for Dialogue: A Short Review](https://arxiv.org/pdf/1611.06216)|Arxiv 2016|Review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.|
|[On the Evaluation of Dialogue Systems with Next Utterance Classification](https://arxiv.org/pdf/1605.05414)|Arxiv 2016|1. Investigate the performance of humans on this task to validate the relevance of Next Utterance Classification (NUC) as a method of evaluation. 2. Results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.|
|[Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders](https://arxiv.org/pdf/1703.10960)|ACL 2017| 1. Present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. The proposed model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. 2. Further develop a novel variant that is integrated with linguistic prior knowledge for better performance. 3. Propose a bag-of-word loss to alleviate the problem of vanishing KL loss.|
|[Learning End-to-End Goal-Oriented Dialog](https://arxiv.org/pdf/1605.07683)|ICLR 2017| 1. Proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. 2. Show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.|
|[Adversarial Learning for Neural Dialogue Generation](https://www.aclweb.org/anthology/D17-1230)|EMNLP 2017| 1. Propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. 2. Cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. 3. Some appealing research directions.|
|[Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models](https://arxiv.org/pdf/1701.03185)|EMNLP 2017| 1. Add self-attention to the decoder to maintain coherence in longer responses, and propose a practical approach, called the glimpse-model, for scaling to large datasets. 2. Introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process.|
|[ParlAI: A Dialog Research Software Platform](http://aclweb.org/anthology/D17-2014)|EMNLP 2017| A unified open-source framework in Python for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others' models, and improving upon existing architectures.|
|[A Network-based End-to-End Trainable Task-oriented Dialogue System](https://arxiv.org/pdf/1604.04562)|EACL 2017| 1. Introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. 2. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand.|
|[Building Task-Oriented Dialogue Systems for Online Shopping](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14261/13975)|AAAI 2017| 1. Present a general solution towards building task-oriented dialogue systems for online shopping, aiming to assist online customers in completing various purchase-related tasks, such as searching products and answering questions, in a natural language conversation manner. 2. Show what & how existing NLP techniques, data resources, and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage.|
|[End-to-End Task-Completion Neural Dialogue Systems](https://arxiv.org/pdf/1703.01008)|IJCNLP 2017| 1. Presents a novel end-to-end learning framework for task-completion dialogue systems. 2. The proposed system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. 3. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system.|
|[A Survey on Dialogue Systems: Recent Advances and New Frontiers](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Poetry Generation](https://pdfs.semanticscholar.org/d89d/053b1c2481088b1af2bd36e0a6d959ff1373.pdf)|SPECOM 2017| 1. Describe a new system for the automatic creation of poetry in Basque that not only generates novel poems, but also creates them conveying a certain attitude or state of mind. 2. The proposed system receives as an input the topic of the poem and the affective state (positive, neutral or negative) and tries to give as output a novel poem that: (1) satisfies formal constraints of rhyme and metric, (2) shows coherent content related to the given topic, and (3) expresses them through the predetermined mood.|
|[Emotional Human-Machine Conversation Generation Based on Long Short-Term Memory](https://link.springer.com/article/10.1007/s12559-017-9539-4)|Cognitive Computation 2017| 1. Propose a new model based on long short-term memory, which is used to achieve an encoder-decoder framework, and we address the emotional factor of conversation generation by changing the modelâ€™s input using a series of input transformations: a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.|
|[A Deep Reinforcement Learning Chatbot](https://arxiv.org/pdf/1709.02349)|Arxiv 2017| 1. Consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. 2. The system has been trained to select an appropriate response from the models in its ensemble.|
|[Building Chatbot with Emotions](http://web.stanford.edu/class/cs224s/reports/Honghao_Wei.pdf)|N.A. 2017| 1. Aims at generating dialogues not only appropriate at content level, but also containing specific emotions. 2. Apply sentimental analysis on the dataset and pick up dialogue with strong emotion. 3. Apply deep reinforcement learning and introduce sentiment rewards during learning phase|
|[Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures](http://aclweb.org/anthology/P18-1133)|ACL 2018|1. Propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. 2. Design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. 3. Propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Augmenting End-to-End Dialogue Systems with Commonsense Knowledge](https://arxiv.org/pdf/1709.05453)|AAAI 2018| 1. Investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. 2. Propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response|
|[A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/pdf/1702.01932)|AAAI 2018| 1. Presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. 2. Generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting.|
|[Neural Response Generation with Dynamic Vocabularies](https://arxiv.org/pdf/1711.11191)|AAAI 2018| 1. Propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. 2. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. 3. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary.|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|
|[Memory-augmented Dialogue Management for Task-oriented Dialogue Systems](https://arxiv.org/pdf/1805.00150)|Arxiv 2018| 1. Propose a novel Memory-Augmented Dialogue management model (MAD) which employs a memory controller and two additional memory structures, i.e., a slot-value memory and an external memory. 2.The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (for instance, cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks through storing more context information. 3. Propose slot-level attention on user utterances to extract specific semantic information for each slot to update the dialogue state efficiently|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjA0MDAxMTYzMywtMTU1NTcyODQsMTk1MD
czOTUyNSwtMTM0ODYyNDkyMiwtMTA4MDU3NjI1NCwtNTgzMTkz
ODU4LC0xODQzODQ2MTc5LDExNzA0MDE4MDcsNTEwMjc4Njk2LC
05NTY3OTExNzRdfQ==
-->